TRTRAINING_DATA_SOURCE=sources/training_data_final_merged.md \
MODEL_TYPE=gpt2 \
USE_LORA=True \
LORA_RANK=16 \
LORA_ALPHA=32.0 \
TRAINING_STEPS=5000 \
LEARNING_RATE=2e-5 \
BLOCK_SIZE=256 \
BATCH_SIZE=8 \
ENABLE_CHECKPOINTS=True \
CHECKPOINT_INTERVAL=500 \
python3 training.py
   üìå GPT-2 fine-tuning: Using LR (2.00e-05) and context (256)
üöÄ FULL MODE: Using production hyperparameters (aggressively optimized for M4)
‚úÖ Using Apple Silicon GPU (Metal Performance Shaders)
Device: mps
Model size: 4 layers, 256 embedding dims, 4 heads
Training data source: sources/training_data_final_merged.md
ü§ñ Using GPT-2 model: gpt2
üîß Using LoRA for efficient fine-tuning
   LoRA rank: 16, alpha: 32.0, dropout: 0.0
üì• Loading gpt2 from HuggingFace...
   Applied LoRA to 49 layers
‚úÖ LoRA adapters applied (rank=16, alpha=32.0)
üìä Parameter Statistics:
   Model: gpt2
   Total parameters: 127,615,504
   Trainable (total): 3,175,696
   Frozen (base model): 124,439,808
   üí∞ LoRA savings: Training only 2.49% of parameters!
Model device: mps:0
‚úÖ Model successfully moved to Apple Silicon GPU (MPS)
‚ÑπÔ∏è  Using MPS without compilation (torch.compile disabled for MPS)
‚úÖ Optimizer initialized with 98 parameter groups (LoRA only)
üìà Learning rate schedule: 500 warmup steps, 4500 decay steps
   LR range: 2.00e-06 ‚Üí 2.00e-05 ‚Üí 2.00e-06
Starting training for 5000 steps...
Batch size: 8, Block size: 256
Vocabulary size: 50,257 tokens
Checkpoints enabled: saving every 500 steps to checkpoints/
--------------------------------------------------
`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.
step 0/5000 (0.0%): train loss 9.2270, val loss 9.2680 | LR: 2.00e-06 | 17.5s (0.00 steps/sec)
step 25/5000 (0.5%) | 0.66 steps/sec | ETA: 126.3m
step 50/5000 (1.0%) | 0.85 steps/sec | ETA: 97.4m
step 75/5000 (1.5%) | 0.90 steps/sec | ETA: 90.9m
step 100/5000 (2.0%) | 0.94 steps/sec | ETA: 86.6m
step 125/5000 (2.5%) | 0.98 steps/sec | ETA: 82.5m
step 150/5000 (3.0%) | 1.01 steps/sec | ETA: 79.9m
step 175/5000 (3.5%) | 1.03 steps/sec | ETA: 77.7m
step 200/5000 (4.0%) | 1.05 steps/sec | ETA: 75.8m
step 225/5000 (4.5%) | 1.07 steps/sec | ETA: 74.4m
step 250/5000 (5.0%) | 1.08 steps/sec | ETA: 73.4m
step 275/5000 (5.5%) | 1.08 steps/sec | ETA: 72.9m
step 300/5000 (6.0%) | 1.08 steps/sec | ETA: 72.5m
step 325/5000 (6.5%) | 1.08 steps/sec | ETA: 72.0m
step 350/5000 (7.0%) | 1.08 steps/sec | ETA: 71.5m
step 375/5000 (7.5%) | 1.09 steps/sec | ETA: 71.0m
step 400/5000 (8.0%) | 1.09 steps/sec | ETA: 70.4m
step 425/5000 (8.5%) | 1.09 steps/sec | ETA: 69.9m
step 450/5000 (9.0%) | 1.09 steps/sec | ETA: 69.6m
step 475/5000 (9.5%) | 1.09 steps/sec | ETA: 69.2m
/Users/grahamnessler/.pyenv/versions/3.10.11/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:198: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
step 500/5000 (10.0%): train loss 6.9544, val loss 7.1291 | LR: 2.00e-05 | 480.9s (1.04 steps/sec)
   üíæ Checkpoint saved: checkpoints/checkpoint_gpt2_training_data_final_merged_step000500_11222025_111021.pt
step 525/5000 (10.5%) | 1.04 steps/sec | ETA: 71.7m
step 550/5000 (11.0%) | 1.04 steps/sec | ETA: 71.3m
step 575/5000 (11.5%) | 1.04 steps/sec | ETA: 70.8m
step 600/5000 (12.0%) | 1.04 steps/sec | ETA: 70.3m
step 625/5000 (12.5%) | 1.04 steps/sec | ETA: 69.8m
step 650/5000 (13.0%) | 1.05 steps/sec | ETA: 69.3m
step 675/5000 (13.5%) | 1.05 steps/sec | ETA: 68.9m
step 700/5000 (14.0%) | 1.05 steps/sec | ETA: 68.4m
step 725/5000 (14.5%) | 1.05 steps/sec | ETA: 68.1m
step 750/5000 (15.0%) | 1.05 steps/sec | ETA: 67.6m
step 775/5000 (15.5%) | 1.05 steps/sec | ETA: 67.2m
step 800/5000 (16.0%) | 1.05 steps/sec | ETA: 66.8m
step 825/5000 (16.5%) | 1.05 steps/sec | ETA: 66.4m
step 850/5000 (17.0%) | 1.05 steps/sec | ETA: 65.9m
step 875/5000 (17.5%) | 1.05 steps/sec | ETA: 65.6m
step 900/5000 (18.0%) | 1.05 steps/sec | ETA: 65.2m
step 925/5000 (18.5%) | 1.05 steps/sec | ETA: 64.8m
step 950/5000 (19.0%) | 1.05 steps/sec | ETA: 64.4m
step 975/5000 (19.5%) | 1.05 steps/sec | ETA: 64.0m
step 1000/5000 (20.0%): train loss 6.5811, val loss 6.7096 | LR: 1.95e-05 | 975.8s (1.02 steps/sec)
   üíæ Checkpoint saved: checkpoints/checkpoint_gpt2_training_data_final_merged_step001000_11222025_111836.pt
step 1025/5000 (20.5%) | 1.02 steps/sec | ETA: 64.7m
step 1050/5000 (21.0%) | 1.02 steps/sec | ETA: 64.3m
step 1075/5000 (21.5%) | 1.02 steps/sec | ETA: 63.9m
step 1100/5000 (22.0%) | 1.02 steps/sec | ETA: 63.6m
step 1125/5000 (22.5%) | 1.02 steps/sec | ETA: 63.3m
step 1150/5000 (23.0%) | 1.02 steps/sec | ETA: 63.1m
step 1175/5000 (23.5%) | 1.01 steps/sec | ETA: 62.9m
step 1200/5000 (24.0%) | 1.01 steps/sec | ETA: 62.5m
step 1225/5000 (24.5%) | 1.01 steps/sec | ETA: 62.1m
step 1250/5000 (25.0%) | 1.01 steps/sec | ETA: 61.7m
step 1275/5000 (25.5%) | 1.01 steps/sec | ETA: 61.3m
step 1300/5000 (26.0%) | 1.01 steps/sec | ETA: 61.0m
step 1325/5000 (26.5%) | 1.01 steps/sec | ETA: 60.6m
step 1350/5000 (27.0%) | 1.01 steps/sec | ETA: 60.2m
step 1375/5000 (27.5%) | 1.01 steps/sec | ETA: 60.0m
step 1400/5000 (28.0%) | 1.01 steps/sec | ETA: 59.5m
step 1425/5000 (28.5%) | 1.01 steps/sec | ETA: 59.1m
step 1450/5000 (29.0%) | 1.01 steps/sec | ETA: 58.8m
step 1475/5000 (29.5%) | 1.01 steps/sec | ETA: 58.4m
step 1500/5000 (30.0%): train loss 6.6511, val loss 6.7427 | LR: 1.79e-05 | 1518.1s (0.99 steps/sec)
   üíæ Checkpoint saved: checkpoints/checkpoint_gpt2_training_data_final_merged_step001500_11222025_112739.pt
step 1525/5000 (30.5%) | 0.99 steps/sec | ETA: 58.8m
step 1550/5000 (31.0%) | 0.98 steps/sec | ETA: 58.5m
step 1575/5000 (31.5%) | 0.98 steps/sec | ETA: 58.1m
step 1600/5000 (32.0%) | 0.98 steps/sec | ETA: 57.7m
step 1625/5000 (32.5%) | 0.98 steps/sec | ETA: 57.5m
step 1650/5000 (33.0%) | 0.98 steps/sec | ETA: 57.2m
step 1675/5000 (33.5%) | 0.97 steps/sec | ETA: 56.9m
step 1700/5000 (34.0%) | 0.97 steps/sec | ETA: 56.7m
step 1725/5000 (34.5%) | 0.97 steps/sec | ETA: 56.4m
step 1750/5000 (35.0%) | 0.96 steps/sec | ETA: 56.2m
step 1775/5000 (35.5%) | 0.96 steps/sec | ETA: 55.8m
step 1800/5000 (36.0%) | 0.96 steps/sec | ETA: 55.4m
step 1825/5000 (36.5%) | 0.96 steps/sec | ETA: 55.0m
step 1850/5000 (37.0%) | 0.96 steps/sec | ETA: 54.5m
step 1875/5000 (37.5%) | 0.96 steps/sec | ETA: 54.1m
step 1900/5000 (38.0%) | 0.96 steps/sec | ETA: 53.7m
step 1925/5000 (38.5%) | 0.96 steps/sec | ETA: 53.2m
step 1950/5000 (39.0%) | 0.96 steps/sec | ETA: 52.8m
step 1975/5000 (39.5%) | 0.96 steps/sec | ETA: 52.3m
step 2000/5000 (40.0%): train loss 9.5846, val loss 9.6169 | LR: 1.55e-05 | 2099.2s (0.95 steps/sec)
   üíæ Checkpoint saved: checkpoints/checkpoint_gpt2_training_data_final_merged_step002000_11222025_113720.pt
step 2025/5000 (40.5%) | 0.95 steps/sec | ETA: 52.1m
step 2050/5000 (41.0%) | 0.95 steps/sec | ETA: 51.7m
step 2075/5000 (41.5%) | 0.95 steps/sec | ETA: 51.4m
step 2100/5000 (42.0%) | 0.95 steps/sec | ETA: 51.0m
step 2125/5000 (42.5%) | 0.95 steps/sec | ETA: 50.5m
step 2150/5000 (43.0%) | 0.95 steps/sec | ETA: 50.2m
step 2175/5000 (43.5%) | 0.95 steps/sec | ETA: 49.7m
step 2200/5000 (44.0%) | 0.95 steps/sec | ETA: 49.3m
step 2225/5000 (44.5%) | 0.95 steps/sec | ETA: 48.8m
step 2250/5000 (45.0%) | 0.95 steps/sec | ETA: 48.4m
step 2275/5000 (45.5%) | 0.95 steps/sec | ETA: 47.9m
step 2300/5000 (46.0%) | 0.95 steps/sec | ETA: 47.5m
step 2325/5000 (46.5%) | 0.95 steps/sec | ETA: 47.1m
step 2350/5000 (47.0%) | 0.69 steps/sec | ETA: 63.9m
step 2375/5000 (47.5%) | 0.61 steps/sec | ETA: 72.3m
step 2400/5000 (48.0%) | 0.61 steps/sec | ETA: 71.3m
step 2425/5000 (48.5%) | 0.61 steps/sec | ETA: 70.2m
step 2450/5000 (49.0%) | 0.61 steps/sec | ETA: 69.2m
step 2475/5000 (49.5%) | 0.62 steps/sec | ETA: 68.2m
step 2500/5000 (50.0%): train loss 6.6424, val loss 6.7105 | LR: 1.26e-05 | 4046.9s (0.62 steps/sec)
   üíæ Checkpoint saved: checkpoints/checkpoint_gpt2_training_data_final_merged_step002500_11222025_120947.pt
step 2525/5000 (50.5%) | 0.62 steps/sec | ETA: 66.5m
step 2550/5000 (51.0%) | 0.62 steps/sec | ETA: 65.5m