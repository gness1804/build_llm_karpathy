HYPERPARAMETERS
================
batch_size = 16
block_size = 128
training_steps = 5000
eval_interval = 500
learning_rate = 2e-05
eval_iters = 20
n_embd = 256
n_head = 4
n_layer = 4
dropout = 0.2
max_new_tokens = 300
device = mps
tokenization_method = character
test_mode = False
use_lora = True
model_type = gpt2
training_data_source = sources/training_data_final_merged.md
gpt2_model_name = gpt2
lora_rank = 16
lora_alpha = 32.0
lora_dropout = 0.0
lr_schedule = constant

OUTPUT
======
   üìå GPT-2 fine-tuning: Using LR (2.00e-05), batch_size=16, block_size=128
üöÄ FULL MODE: Using production hyperparameters (aggressively optimized for M4)
‚úÖ Using Apple Silicon GPU (Metal Performance Shaders)
ü§ñ Using GPT-2 model: gpt2
üîß Using LoRA for efficient fine-tuning
   LoRA rank: 16, alpha: 32.0, dropout: 0.0
üì• Loading gpt2 from HuggingFace...
   Applied LoRA to 49 layers
‚úÖ LoRA adapters applied (rank=16, alpha=32.0)
üìä Parameter Statistics:
   Model: gpt2
   Total parameters: 127,615,504
   Trainable (total): 3,175,696
   Frozen (base model): 124,439,808
   üí∞ LoRA savings: Training only 2.49% of parameters!
Model device: mps:0
‚úÖ Model successfully moved to Apple Silicon GPU (MPS)
‚ÑπÔ∏è  Using MPS without compilation (torch.compile disabled for MPS)
‚úÖ Optimizer initialized with 98 parameter groups (LoRA only)
üìà Learning rate schedule: CONSTANT (warmup disabled via USE_LR_WARMUP=False)
   LR: 2.00e-05 (constant throughout training)
Starting training for 5000 steps...
Batch size: 16, Block size: 128
Vocabulary size: 50,257 tokens
Checkpoints enabled: saving every 500 steps to checkpoints/
--------------------------------------------------
üìù Checkpoint log: logs/training_log_gpt2_training_data_final_merged_11222025_201917.log
step 0/5000 (0.0%): train loss 9.2871, val loss 9.2992 | LR: 2.00e-05 | 17.3s (0.00 steps/sec) | Net loss change since beginning: 0.0000 | Net loss change since last checkpoint: 0.0000
step 25/5000 (0.5%) | 0.68 steps/sec | ETA: 122.2m
step 50/5000 (1.0%) | 0.89 steps/sec | ETA: 92.5m
step 75/5000 (1.5%) | 1.00 steps/sec | ETA: 82.5m
step 100/5000 (2.0%) | 1.05 steps/sec | ETA: 77.6m
step 125/5000 (2.5%) | 1.08 steps/sec | ETA: 75.2m
step 150/5000 (3.0%) | 1.09 steps/sec | ETA: 74.4m
step 175/5000 (3.5%) | 1.07 steps/sec | ETA: 75.2m
step 200/5000 (4.0%) | 1.06 steps/sec | ETA: 75.8m
step 225/5000 (4.5%) | 1.05 steps/sec | ETA: 76.2m
step 250/5000 (5.0%) | 1.03 steps/sec | ETA: 76.5m
step 275/5000 (5.5%) | 1.03 steps/sec | ETA: 76.2m
step 300/5000 (6.0%) | 1.03 steps/sec | ETA: 75.7m
step 325/5000 (6.5%) | 1.04 steps/sec | ETA: 75.3m
step 350/5000 (7.0%) | 1.03 steps/sec | ETA: 75.2m
step 375/5000 (7.5%) | 1.03 steps/sec | ETA: 74.9m
step 400/5000 (8.0%) | 1.02 steps/sec | ETA: 75.1m
step 425/5000 (8.5%) | 1.01 steps/sec | ETA: 75.3m
step 450/5000 (9.0%) | 1.00 steps/sec | ETA: 75.5m
step 475/5000 (9.5%) | 1.00 steps/sec | ETA: 75.7m
step 500/5000 (10.0%): train loss 6.6119, val loss 6.7544 | LR: 2.00e-05 | 530.5s (0.94 steps/sec) | Net loss change since beginning: -2.6752 | Net loss change since last checkpoint: -2.6752
   üíæ Checkpoint saved: checkpoints/checkpoint_gpt2_training_data_final_merged_step000500_11222025_202808.pt
step 525/5000 (10.5%) | 0.94 steps/sec | ETA: 79.2m
step 550/5000 (11.0%) | 0.94 steps/sec | ETA: 78.5m
step 575/5000 (11.5%) | 0.94 steps/sec | ETA: 78.1m
step 600/5000 (12.0%) | 0.95 steps/sec | ETA: 77.5m
step 625/5000 (12.5%) | 0.95 steps/sec | ETA: 76.9m
step 650/5000 (13.0%) | 0.95 steps/sec | ETA: 76.3m
step 675/5000 (13.5%) | 0.95 steps/sec | ETA: 75.8m
step 700/5000 (14.0%) | 0.40 steps/sec | ETA: 178.8m
step 725/5000 (14.5%) | 0.41 steps/sec | ETA: 175.4m
step 750/5000 (15.0%) | 0.27 steps/sec | ETA: 260.5m
step 775/5000 (15.5%) | 0.28 steps/sec | ETA: 252.4m
step 800/5000 (16.0%) | 0.29 steps/sec | ETA: 244.7m
step 825/5000 (16.5%) | 0.29 steps/sec | ETA: 237.5m
step 850/5000 (17.0%) | 0.30 steps/sec | ETA: 230.6m
step 875/5000 (17.5%) | 0.31 steps/sec | ETA: 224.2m
step 900/5000 (18.0%) | 0.31 steps/sec | ETA: 218.1m
step 925/5000 (18.5%) | 0.32 steps/sec | ETA: 212.3m
step 950/5000 (19.0%) | 0.33 steps/sec | ETA: 206.8m
step 975/5000 (19.5%) | 0.33 steps/sec | ETA: 201.5m
step 1000/5000 (20.0%): train loss 6.6722, val loss 6.7246 | LR: 2.00e-05 | 2965.7s (0.34 steps/sec) | Net loss change since beginning: -2.6149 | Net loss change since last checkpoint: 0.0602
   üíæ Checkpoint saved: checkpoints/checkpoint_gpt2_training_data_final_merged_step001000_11222025_210843.pt
step 1025/5000 (20.5%) | 0.34 steps/sec | ETA: 193.0m
step 1050/5000 (21.0%) | 0.35 steps/sec | ETA: 188.6m
step 1075/5000 (21.5%) | 0.35 steps/sec | ETA: 184.3m
step 1100/5000 (22.0%) | 0.36 steps/sec | ETA: 180.3m
step 1125/5000 (22.5%) | 0.37 steps/sec | ETA: 176.5m
step 1150/5000 (23.0%) | 0.37 steps/sec | ETA: 173.0m
step 1175/5000 (23.5%) | 0.38 steps/sec | ETA: 169.6m
step 1200/5000 (24.0%) | 0.38 steps/sec | ETA: 166.3m
step 1225/5000 (24.5%) | 0.39 steps/sec | ETA: 163.1m
step 1250/5000 (25.0%) | 0.39 steps/sec | ETA: 160.0m
step 1275/5000 (25.5%) | 0.40 steps/sec | ETA: 157.0m
step 1300/5000 (26.0%) | 0.40 steps/sec | ETA: 154.2m
step 1325/5000 (26.5%) | 0.40 steps/sec | ETA: 151.4m
step 1350/5000 (27.0%) | 0.41 steps/sec | ETA: 148.8m
step 1375/5000 (27.5%) | 0.41 steps/sec | ETA: 146.2m
step 1400/5000 (28.0%) | 0.42 steps/sec | ETA: 143.7m
step 1425/5000 (28.5%) | 0.34 steps/sec | ETA: 176.4m
step 1450/5000 (29.0%) | 0.34 steps/sec | ETA: 172.9m
step 1475/5000 (29.5%) | 0.35 steps/sec | ETA: 169.6m
step 1500/5000 (30.0%): train loss 7.1735, val loss 7.2504 | LR: 2.00e-05 | 4293.5s (0.35 steps/sec) | Net loss change since beginning: -2.1136 | Net loss change since last checkpoint: 0.5013
   üíæ Checkpoint saved: checkpoints/checkpoint_gpt2_training_data_final_merged_step001500_11222025_213051.pt
step 1525/5000 (30.5%) | 0.35 steps/sec | ETA: 163.8m
step 1550/5000 (31.0%) | 0.36 steps/sec | ETA: 160.7m
step 1575/5000 (31.5%) | 0.36 steps/sec | ETA: 157.7m
step 1600/5000 (32.0%) | 0.37 steps/sec | ETA: 154.8m
step 1625/5000 (32.5%) | 0.37 steps/sec | ETA: 152.0m
step 1650/5000 (33.0%) | 0.37 steps/sec | ETA: 149.2m
step 1675/5000 (33.5%) | 0.38 steps/sec | ETA: 146.6m
step 1700/5000 (34.0%) | 0.38 steps/sec | ETA: 144.1m
step 1725/5000 (34.5%) | 0.39 steps/sec | ETA: 141.6m
step 1750/5000 (35.0%) | 0.39 steps/sec | ETA: 139.2m
step 1775/5000 (35.5%) | 0.39 steps/sec | ETA: 136.9m
step 1800/5000 (36.0%) | 0.40 steps/sec | ETA: 134.6m
step 1825/5000 (36.5%) | 0.40 steps/sec | ETA: 132.4m
step 1850/5000 (37.0%) | 0.40 steps/sec | ETA: 130.3m
step 1875/5000 (37.5%) | 0.41 steps/sec | ETA: 128.3m
step 1900/5000 (38.0%) | 0.41 steps/sec | ETA: 126.3m
step 1925/5000 (38.5%) | 0.41 steps/sec | ETA: 124.3m
step 1950/5000 (39.0%) | 0.42 steps/sec | ETA: 122.4m
step 1975/5000 (39.5%) | 0.42 steps/sec | ETA: 120.5m
step 2000/5000 (40.0%): train loss 6.8297, val loss 6.9316 | LR: 2.00e-05 | 4772.0s (0.42 steps/sec) | Net loss change since beginning: -2.4574 | Net loss change since last checkpoint: -0.3438
   üíæ Checkpoint saved: checkpoints/checkpoint_gpt2_training_data_final_merged_step002000_11222025_213849.pt
step 2025/5000 (40.5%) | 0.42 steps/sec | ETA: 117.5m
step 2050/5000 (41.0%) | 0.42 steps/sec | ETA: 115.8m
step 2075/5000 (41.5%) | 0.43 steps/sec | ETA: 114.0m
step 2100/5000 (42.0%) | 0.36 steps/sec | ETA: 136.0m
step 2125/5000 (42.5%) | 0.30 steps/sec | ETA: 157.5m
step 2150/5000 (43.0%) | 0.27 steps/sec | ETA: 173.8m
step 2175/5000 (43.5%) | 0.28 steps/sec | ETA: 170.7m
step 2200/5000 (44.0%) | 0.28 steps/sec | ETA: 167.7m
step 2225/5000 (44.5%) | 0.28 steps/sec | ETA: 164.7m
step 2250/5000 (45.0%) | 0.28 steps/sec | ETA: 161.8m
step 2275/5000 (45.5%) | 0.29 steps/sec | ETA: 158.9m
step 2300/5000 (46.0%) | 0.29 steps/sec | ETA: 156.1m
step 2325/5000 (46.5%) | 0.29 steps/sec | ETA: 153.4m
step 2350/5000 (47.0%) | 0.29 steps/sec | ETA: 150.7m
step 2375/5000 (47.5%) | 0.30 steps/sec | ETA: 148.1m
step 2400/5000 (48.0%) | 0.30 steps/sec | ETA: 145.5m
step 2425/5000 (48.5%) | 0.30 steps/sec | ETA: 142.9m
step 2450/5000 (49.0%) | 0.30 steps/sec | ETA: 140.4m
step 2475/5000 (49.5%) | 0.31 steps/sec | ETA: 138.0m
step 2500/5000 (50.0%): train loss 6.5804, val loss 6.7412 | LR: 2.00e-05 | 8152.2s (0.31 steps/sec) | Net loss change since beginning: -2.7067 | Net loss change since last checkpoint: -0.2493
   üíæ Checkpoint saved: checkpoints/checkpoint_gpt2_training_data_final_merged_step002500_11222025_223510.pt
step 2525/5000 (50.5%) | 0.31 steps/sec | ETA: 133.5m
step 2550/5000 (51.0%) | 0.31 steps/sec | ETA: 131.3m
step 2575/5000 (51.5%) | 0.31 steps/sec | ETA: 129.0m
step 2600/5000 (52.0%) | 0.32 steps/sec | ETA: 126.9m
step 2625/5000 (52.5%) | 0.32 steps/sec | ETA: 124.7m
step 2650/5000 (53.0%) | 0.32 steps/sec | ETA: 122.6m
step 2675/5000 (53.5%) | 0.32 steps/sec | ETA: 120.5m
step 2700/5000 (54.0%) | 0.32 steps/sec | ETA: 118.4m
step 2725/5000 (54.5%) | 0.33 steps/sec | ETA: 116.4m
step 2750/5000 (55.0%) | 0.33 steps/sec | ETA: 114.3m
step 2775/5000 (55.5%) | 0.33 steps/sec | ETA: 112.3m
step 2800/5000 (56.0%) | 0.33 steps/sec | ETA: 110.4m
step 2825/5000 (56.5%) | 0.33 steps/sec | ETA: 108.4m
step 2850/5000 (57.0%) | 0.34 steps/sec | ETA: 106.5m
step 2875/5000 (57.5%) | 0.30 steps/sec | ETA: 116.6m
step 2900/5000 (58.0%) | 0.28 steps/sec | ETA: 125.9m
step 2925/5000 (58.5%) | 0.28 steps/sec | ETA: 123.8m
step 2950/5000 (59.0%) | 0.28 steps/sec | ETA: 121.8m
step 2975/5000 (59.5%) | 0.28 steps/sec | ETA: 119.8m
step 3000/5000 (60.0%): train loss 6.7087, val loss 6.8902 | LR: 2.00e-05 | 11573.1s (0.26 steps/sec) | Net loss change since beginning: -2.5784 | Net loss change since last checkpoint: 0.1283
   üíæ Checkpoint saved: checkpoints/checkpoint_gpt2_training_data_final_merged_step003000_11222025_233211.pt
step 3025/5000 (60.5%) | 0.24 steps/sec | ETA: 136.6m
step 3050/5000 (61.0%) | 0.23 steps/sec | ETA: 143.9m
step 3075/5000 (61.5%) | 0.21 steps/sec | ETA: 151.5m
step 3100/5000 (62.0%) | 0.21 steps/sec | ETA: 148.7m
step 3125/5000 (62.5%) | 0.20 steps/sec | ETA: 154.6m
step 3150/5000 (63.0%) | 0.19 steps/sec | ETA: 160.5m
step 3175/5000 (63.5%) | 0.18 steps/sec | ETA: 166.3m
step 3200/5000 (64.0%) | 0.18 steps/sec | ETA: 163.1m
step 3225/5000 (64.5%) | 0.18 steps/sec | ETA: 168.7m
step 3250/5000 (65.0%) | 0.17 steps/sec | ETA: 171.5m
step 3275/5000 (65.5%) | 0.16 steps/sec | ETA: 176.9m
step 3300/5000 (66.0%) | 0.16 steps/sec | ETA: 181.5m
step 3325/5000 (66.5%) | 0.16 steps/sec | ETA: 177.8m
step 3350/5000 (67.0%) | 0.15 steps/sec | ETA: 181.6m
step 3375/5000 (67.5%) | 0.15 steps/sec | ETA: 182.7m
step 3400/5000 (68.0%) | 0.14 steps/sec | ETA: 186.2m
step 3425/5000 (68.5%) | 0.14 steps/sec | ETA: 189.6m
step 3450/5000 (69.0%) | 0.13 steps/sec | ETA: 192.6m
step 3475/5000 (69.5%) | 0.13 steps/sec | ETA: 188.4m
step 3500/5000 (70.0%): train loss 6.9173, val loss 7.0989 | LR: 2.00e-05 | 27451.8s (0.13 steps/sec) | Net loss change since beginning: -2.3698 | Net loss change since last checkpoint: 0.2086
   üíæ Checkpoint saved: checkpoints/checkpoint_gpt2_training_data_final_merged_step003500_11232025_035649.pt
step 3525/5000 (70.5%) | 0.12 steps/sec | ETA: 198.3m
step 3550/5000 (71.0%) | 0.12 steps/sec | ETA: 193.9m
step 3575/5000 (71.5%) | 0.13 steps/sec | ETA: 189.5m
step 3600/5000 (72.0%) | 0.13 steps/sec | ETA: 185.1m
step 3625/5000 (72.5%) | 0.13 steps/sec | ETA: 180.8m
step 3650/5000 (73.0%) | 0.13 steps/sec | ETA: 176.6m
step 3675/5000 (73.5%) | 0.13 steps/sec | ETA: 172.4m
step 3700/5000 (74.0%) | 0.13 steps/sec | ETA: 168.2m
step 3725/5000 (74.5%) | 0.13 steps/sec | ETA: 164.1m
step 3750/5000 (75.0%) | 0.13 steps/sec | ETA: 160.0m
step 3775/5000 (75.5%) | 0.13 steps/sec | ETA: 156.0m
step 3800/5000 (76.0%) | 0.13 steps/sec | ETA: 152.1m
step 3825/5000 (76.5%) | 0.13 steps/sec | ETA: 148.1m
step 3850/5000 (77.0%) | 0.13 steps/sec | ETA: 144.2m
step 3875/5000 (77.5%) | 0.13 steps/sec | ETA: 140.4m
step 3900/5000 (78.0%) | 0.13 steps/sec | ETA: 136.6m
step 3925/5000 (78.5%) | 0.13 steps/sec | ETA: 132.8m
step 3950/5000 (79.0%) | 0.14 steps/sec | ETA: 129.1m
step 3975/5000 (79.5%) | 0.14 steps/sec | ETA: 125.4m
step 4000/5000 (80.0%): train loss 6.6406, val loss 6.7582 | LR: 2.00e-05 | 29266.9s (0.14 steps/sec) | Net loss change since beginning: -2.6465 | Net loss change since last checkpoint: -0.2767
   üíæ Checkpoint saved: checkpoints/checkpoint_gpt2_training_data_final_merged_step004000_11232025_042704.pt
step 4025/5000 (80.5%) | 0.14 steps/sec | ETA: 118.3m
step 4050/5000 (81.0%) | 0.14 steps/sec | ETA: 114.7m
step 4075/5000 (81.5%) | 0.14 steps/sec | ETA: 111.2m
step 4100/5000 (82.0%) | 0.14 steps/sec | ETA: 107.7m
step 4125/5000 (82.5%) | 0.14 steps/sec | ETA: 104.2m
step 4150/5000 (83.0%) | 0.14 steps/sec | ETA: 100.7m
step 4175/5000 (83.5%) | 0.14 steps/sec | ETA: 97.3m
step 4200/5000 (84.0%) | 0.14 steps/sec | ETA: 93.9m
step 4225/5000 (84.5%) | 0.14 steps/sec | ETA: 90.6m
step 4250/5000 (85.0%) | 0.14 steps/sec | ETA: 87.3m
step 4275/5000 (85.5%) | 0.14 steps/sec | ETA: 85.0m
step 4300/5000 (86.0%) | 0.14 steps/sec | ETA: 84.2m
step 4325/5000 (86.5%) | 0.14 steps/sec | ETA: 80.9m
step 4350/5000 (87.0%) | 0.14 steps/sec | ETA: 77.5m
step 4375/5000 (87.5%) | 0.14 steps/sec | ETA: 74.2m
step 4400/5000 (88.0%) | 0.14 steps/sec | ETA: 70.9m
step 4425/5000 (88.5%) | 0.14 steps/sec | ETA: 67.7m
step 4450/5000 (89.0%) | 0.14 steps/sec | ETA: 64.4m
step 4475/5000 (89.5%) | 0.14 steps/sec | ETA: 61.2m
step 4500/5000 (90.0%): train loss 6.6403, val loss 6.8589 | LR: 2.00e-05 | 31396.3s (0.14 steps/sec) | Net loss change since beginning: -2.6468 | Net loss change since last checkpoint: -0.0003
   üíæ Checkpoint saved: checkpoints/checkpoint_gpt2_training_data_final_merged_step004500_11232025_050234.pt
step 4525/5000 (90.5%) | 0.14 steps/sec | ETA: 55.0m
step 4550/5000 (91.0%) | 0.14 steps/sec | ETA: 51.9m
step 4575/5000 (91.5%) | 0.15 steps/sec | ETA: 48.8m
step 4600/5000 (92.0%) | 0.15 steps/sec | ETA: 45.7m
step 4625/5000 (92.5%) | 0.15 steps/sec | ETA: 42.7m
step 4650/5000 (93.0%) | 0.15 steps/sec | ETA: 39.7m
step 4675/5000 (93.5%) | 0.15 steps/sec | ETA: 36.7m
step 4700/5000 (94.0%) | 0.15 steps/sec | ETA: 33.7m
step 4725/5000 (94.5%) | 0.15 steps/sec | ETA: 30.8m
step 4750/5000 (95.0%) | 0.15 steps/sec | ETA: 27.9m
step 4775/5000 (95.5%) | 0.15 steps/sec | ETA: 25.0m
step 4800/5000 (96.0%) | 0.15 steps/sec | ETA: 22.1m
step 4825/5000 (96.5%) | 0.15 steps/sec | ETA: 19.3m
step 4850/5000 (97.0%) | 0.15 steps/sec | ETA: 16.5m
step 4875/5000 (97.5%) | 0.15 steps/sec | ETA: 13.7m
step 4900/5000 (98.0%) | 0.15 steps/sec | ETA: 10.9m
step 4925/5000 (98.5%) | 0.15 steps/sec | ETA: 8.1m
step 4950/5000 (99.0%) | 0.15 steps/sec | ETA: 5.4m
step 4975/5000 (99.5%) | 0.15 steps/sec | ETA: 2.7m
--------------------------------------------------
Training complete! Final loss: 7.5484
Total training time: 8h 55m 53s (0.16 steps/sec)
‚úÖ Final model saved: checkpoints/checkpoint_gpt2_training_data_final_merged_step005000_11232025_051511.pt
üìù Checkpoint log saved: logs/training_log_gpt2_training_data_final_merged_11222025_201917.log

Generating text...
==================================================
ÔøΩt a you I in I about a. and IÔøΩt.., the.

s to and it, when had and a for it I to.., her and the to all I my this I he I a the to I she it to to I this to the., like to to you do is. to the. to.... I't of. I to about the to the and in, the to and it,,, he with she me in that so was was and all want in, to and that be want the him out a when....
? and on
_ I to of of,ÔøΩt like on was it and he wasÔøΩt a don it the I with a I he in. I a., theÔøΩs. I./: my.F,
m about for it this and with to to I to to,, to and the my of and her with want he is in of. my don to on, and his when. the a to, I the he. he, a this to, I how. (ÔøΩt to,.
m.://:ÔøΩt,
t redditWERÔøΩt do and my and the to.QUESTs I.ION://\s in
m.
d in's so.
m, is in
m.So.My andÔøΩt I and, is this.I was,.QUEST
==================================================
