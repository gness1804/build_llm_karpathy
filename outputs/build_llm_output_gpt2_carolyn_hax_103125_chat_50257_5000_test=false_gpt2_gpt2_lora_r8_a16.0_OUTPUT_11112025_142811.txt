HYPERPARAMETERS
================
batch_size = 16
block_size = 64
training_steps = 5000
eval_interval = 500
learning_rate = 0.0003
eval_iters = 20
n_embd = 256
n_head = 4
n_layer = 4
dropout = 0.2
max_new_tokens = 300
device = mps
tokenization_method = character
test_mode = False
use_lora = True
model_type = gpt2
training_data_source = sources/carolyn_hax_103125_chat.md
gpt2_model_name = gpt2
lora_rank = 8
lora_alpha = 16.0
lora_dropout = 0.0

OUTPUT
======
üöÄ FULL MODE: Using production hyperparameters (aggressively optimized for M4)
   üìå GPT-2 detected: Using optimized batch/block sizes for MPS
‚úÖ Using Apple Silicon GPU (Metal Performance Shaders)
Device: mps
Model size: 4 layers, 256 embedding dims, 4 heads
ü§ñ Using GPT-2 model: gpt2
üîß Using LoRA for efficient fine-tuning
   LoRA rank: 8, alpha: 16.0, dropout: 0.0
üì• Loading gpt2 from HuggingFace...
   Applied LoRA to 49 layers
‚úÖ LoRA adapters applied (rank=8, alpha=16.0)
üìä Parameter Statistics:
   Model: gpt2
   Total parameters: 126,027,656
   Trainable (total): 1,587,848
   Frozen (base model): 124,439,808
   üí∞ LoRA savings: Training only 1.26% of parameters!
Model device: mps:0
‚úÖ Model successfully moved to Apple Silicon GPU (MPS)
‚ÑπÔ∏è  Using MPS without compilation (torch.compile disabled for MPS)
‚úÖ Optimizer initialized with 98 parameter groups (LoRA only)
Starting training for 5000 steps...
Batch size: 16, Block size: 64
Vocabulary size: 50,257 tokens
Checkpoints enabled: saving every 500 steps to checkpoints/
--------------------------------------------------
step 0/5000 (0.0%): train loss 9.5012, val loss 9.5813 | 8.2s (0.00 steps/sec)
step 25/5000 (0.5%) | 1.49 steps/sec | ETA: 55.6m
step 50/5000 (1.0%) | 1.97 steps/sec | ETA: 41.8m
step 75/5000 (1.5%) | 2.21 steps/sec | ETA: 37.1m
step 100/5000 (2.0%) | 2.36 steps/sec | ETA: 34.6m
step 125/5000 (2.5%) | 2.45 steps/sec | ETA: 33.1m
step 150/5000 (3.0%) | 2.52 steps/sec | ETA: 32.1m
step 175/5000 (3.5%) | 2.57 steps/sec | ETA: 31.3m
step 200/5000 (4.0%) | 2.61 steps/sec | ETA: 30.6m
step 225/5000 (4.5%) | 2.64 steps/sec | ETA: 30.1m
step 250/5000 (5.0%) | 2.67 steps/sec | ETA: 29.7m
step 275/5000 (5.5%) | 2.69 steps/sec | ETA: 29.3m
step 300/5000 (6.0%) | 2.71 steps/sec | ETA: 28.9m
step 325/5000 (6.5%) | 2.72 steps/sec | ETA: 28.6m
step 350/5000 (7.0%) | 2.74 steps/sec | ETA: 28.3m
step 375/5000 (7.5%) | 2.75 steps/sec | ETA: 28.0m
step 400/5000 (8.0%) | 2.76 steps/sec | ETA: 27.8m
step 425/5000 (8.5%) | 2.77 steps/sec | ETA: 27.5m
step 450/5000 (9.0%) | 2.78 steps/sec | ETA: 27.3m
step 475/5000 (9.5%) | 2.78 steps/sec | ETA: 27.1m
step 500/5000 (10.0%): train loss 5.8200, val loss 6.5911 | 187.2s (2.67 steps/sec)
   üíæ Checkpoint saved: checkpoints/checkpoint_gpt2_carolyn_hax_103125_chat_step000500_11112025_131807.pt
step 525/5000 (10.5%) | 2.67 steps/sec | ETA: 28.0m
step 550/5000 (11.0%) | 2.68 steps/sec | ETA: 27.7m
step 575/5000 (11.5%) | 2.69 steps/sec | ETA: 27.4m
step 600/5000 (12.0%) | 2.70 steps/sec | ETA: 27.2m
step 625/5000 (12.5%) | 2.70 steps/sec | ETA: 27.0m
step 650/5000 (13.0%) | 2.71 steps/sec | ETA: 26.7m
step 675/5000 (13.5%) | 2.72 steps/sec | ETA: 26.5m
step 700/5000 (14.0%) | 2.72 steps/sec | ETA: 26.3m
step 725/5000 (14.5%) | 2.73 steps/sec | ETA: 26.1m
step 750/5000 (15.0%) | 2.73 steps/sec | ETA: 25.9m
step 775/5000 (15.5%) | 2.74 steps/sec | ETA: 25.7m
step 800/5000 (16.0%) | 2.74 steps/sec | ETA: 25.6m
step 825/5000 (16.5%) | 2.74 steps/sec | ETA: 25.4m
step 850/5000 (17.0%) | 2.74 steps/sec | ETA: 25.3m
step 875/5000 (17.5%) | 2.73 steps/sec | ETA: 25.2m
step 900/5000 (18.0%) | 2.73 steps/sec | ETA: 25.1m
step 925/5000 (18.5%) | 2.72 steps/sec | ETA: 25.0m
step 950/5000 (19.0%) | 2.71 steps/sec | ETA: 24.9m
step 975/5000 (19.5%) | 2.71 steps/sec | ETA: 24.8m
step 1000/5000 (20.0%): train loss 5.8790, val loss 6.9977 | 380.2s (2.63 steps/sec)
   üíæ Checkpoint saved: checkpoints/checkpoint_gpt2_carolyn_hax_103125_chat_step001000_11112025_132120.pt
step 1025/5000 (20.5%) | 2.62 steps/sec | ETA: 25.3m
step 1050/5000 (21.0%) | 2.62 steps/sec | ETA: 25.2m
step 1075/5000 (21.5%) | 2.61 steps/sec | ETA: 25.0m
step 1100/5000 (22.0%) | 2.61 steps/sec | ETA: 24.9m
step 1125/5000 (22.5%) | 2.61 steps/sec | ETA: 24.8m
step 1150/5000 (23.0%) | 2.61 steps/sec | ETA: 24.6m
step 1175/5000 (23.5%) | 2.61 steps/sec | ETA: 24.4m
step 1200/5000 (24.0%) | 2.61 steps/sec | ETA: 24.3m
step 1225/5000 (24.5%) | 2.61 steps/sec | ETA: 24.1m
step 1250/5000 (25.0%) | 2.61 steps/sec | ETA: 23.9m
step 1275/5000 (25.5%) | 2.62 steps/sec | ETA: 23.7m
step 1300/5000 (26.0%) | 2.62 steps/sec | ETA: 23.5m
step 1325/5000 (26.5%) | 2.62 steps/sec | ETA: 23.4m
step 1350/5000 (27.0%) | 2.62 steps/sec | ETA: 23.2m
step 1375/5000 (27.5%) | 2.63 steps/sec | ETA: 23.0m
step 1400/5000 (28.0%) | 2.63 steps/sec | ETA: 22.8m
step 1425/5000 (28.5%) | 2.63 steps/sec | ETA: 22.7m
step 1450/5000 (29.0%) | 2.63 steps/sec | ETA: 22.5m
step 1475/5000 (29.5%) | 2.63 steps/sec | ETA: 22.3m
step 1500/5000 (30.0%): train loss 5.5917, val loss 7.1702 | 578.6s (2.59 steps/sec)
   üíæ Checkpoint saved: checkpoints/checkpoint_gpt2_carolyn_hax_103125_chat_step001500_11112025_132439.pt
step 1525/5000 (30.5%) | 2.59 steps/sec | ETA: 22.4m
step 1550/5000 (31.0%) | 2.59 steps/sec | ETA: 22.2m
step 1575/5000 (31.5%) | 2.59 steps/sec | ETA: 22.0m
step 1600/5000 (32.0%) | 2.60 steps/sec | ETA: 21.8m
step 1625/5000 (32.5%) | 2.60 steps/sec | ETA: 21.7m
step 1650/5000 (33.0%) | 2.60 steps/sec | ETA: 21.5m
step 1675/5000 (33.5%) | 2.60 steps/sec | ETA: 21.3m
step 1700/5000 (34.0%) | 2.60 steps/sec | ETA: 21.1m
step 1725/5000 (34.5%) | 2.60 steps/sec | ETA: 21.0m
step 1750/5000 (35.0%) | 2.60 steps/sec | ETA: 20.8m
step 1775/5000 (35.5%) | 2.60 steps/sec | ETA: 20.6m
step 1800/5000 (36.0%) | 2.60 steps/sec | ETA: 20.5m
step 1825/5000 (36.5%) | 2.60 steps/sec | ETA: 20.3m
step 1850/5000 (37.0%) | 2.61 steps/sec | ETA: 20.1m
step 1875/5000 (37.5%) | 2.61 steps/sec | ETA: 20.0m
step 1900/5000 (38.0%) | 2.61 steps/sec | ETA: 19.8m
step 1925/5000 (38.5%) | 2.61 steps/sec | ETA: 19.6m
step 1950/5000 (39.0%) | 2.61 steps/sec | ETA: 19.5m
step 1975/5000 (39.5%) | 2.60 steps/sec | ETA: 19.4m
step 2000/5000 (40.0%): train loss 5.2800, val loss 6.8063 | 780.3s (2.56 steps/sec)
   üíæ Checkpoint saved: checkpoints/checkpoint_gpt2_carolyn_hax_103125_chat_step002000_11112025_132801.pt
step 2025/5000 (40.5%) | 2.56 steps/sec | ETA: 19.4m
step 2050/5000 (41.0%) | 2.56 steps/sec | ETA: 19.2m
step 2075/5000 (41.5%) | 2.55 steps/sec | ETA: 19.1m
step 2100/5000 (42.0%) | 2.55 steps/sec | ETA: 18.9m
step 2125/5000 (42.5%) | 2.55 steps/sec | ETA: 18.8m
step 2150/5000 (43.0%) | 2.55 steps/sec | ETA: 18.6m
step 2175/5000 (43.5%) | 2.55 steps/sec | ETA: 18.5m
step 2200/5000 (44.0%) | 2.54 steps/sec | ETA: 18.3m
step 2225/5000 (44.5%) | 2.54 steps/sec | ETA: 18.2m
step 2250/5000 (45.0%) | 2.54 steps/sec | ETA: 18.0m
step 2275/5000 (45.5%) | 2.54 steps/sec | ETA: 17.9m
step 2300/5000 (46.0%) | 2.54 steps/sec | ETA: 17.7m
step 2325/5000 (46.5%) | 2.53 steps/sec | ETA: 17.6m
step 2350/5000 (47.0%) | 2.53 steps/sec | ETA: 17.4m
step 2375/5000 (47.5%) | 2.53 steps/sec | ETA: 17.3m
step 2400/5000 (48.0%) | 2.53 steps/sec | ETA: 17.1m
step 2425/5000 (48.5%) | 2.53 steps/sec | ETA: 17.0m
step 2450/5000 (49.0%) | 2.52 steps/sec | ETA: 16.8m
step 2475/5000 (49.5%) | 2.52 steps/sec | ETA: 16.7m
step 2500/5000 (50.0%): train loss 5.1780, val loss 6.9061 | 1002.4s (2.49 steps/sec)
   üíæ Checkpoint saved: checkpoints/checkpoint_gpt2_carolyn_hax_103125_chat_step002500_11112025_133143.pt
step 2525/5000 (50.5%) | 2.49 steps/sec | ETA: 16.6m
step 2550/5000 (51.0%) | 2.49 steps/sec | ETA: 16.4m
step 2575/5000 (51.5%) | 2.49 steps/sec | ETA: 16.3m
step 2600/5000 (52.0%) | 2.48 steps/sec | ETA: 16.1m
step 2625/5000 (52.5%) | 2.47 steps/sec | ETA: 16.0m
step 2650/5000 (53.0%) | 1.27 steps/sec | ETA: 30.9m
step 2675/5000 (53.5%) | 1.27 steps/sec | ETA: 30.6m
step 2700/5000 (54.0%) | 1.27 steps/sec | ETA: 30.3m
step 2725/5000 (54.5%) | 1.27 steps/sec | ETA: 30.0m
step 2750/5000 (55.0%) | 1.26 steps/sec | ETA: 29.7m
step 2775/5000 (55.5%) | 1.26 steps/sec | ETA: 29.3m
step 2800/5000 (56.0%) | 1.26 steps/sec | ETA: 29.0m
step 2825/5000 (56.5%) | 1.26 steps/sec | ETA: 28.7m
step 2850/5000 (57.0%) | 1.26 steps/sec | ETA: 28.4m
step 2875/5000 (57.5%) | 1.26 steps/sec | ETA: 28.0m
step 2900/5000 (58.0%) | 1.26 steps/sec | ETA: 27.7m
step 2925/5000 (58.5%) | 1.26 steps/sec | ETA: 27.4m
step 2950/5000 (59.0%) | 1.26 steps/sec | ETA: 27.1m
step 2975/5000 (59.5%) | 1.26 steps/sec | ETA: 26.7m
step 3000/5000 (60.0%): train loss 5.9209, val loss 7.2923 | 2399.8s (1.25 steps/sec)
   üíæ Checkpoint saved: checkpoints/checkpoint_gpt2_carolyn_hax_103125_chat_step003000_11112025_135500.pt
step 3025/5000 (60.5%) | 1.25 steps/sec | ETA: 26.4m
step 3050/5000 (61.0%) | 1.25 steps/sec | ETA: 26.0m
step 3075/5000 (61.5%) | 1.25 steps/sec | ETA: 25.7m
step 3100/5000 (62.0%) | 1.25 steps/sec | ETA: 25.4m
step 3125/5000 (62.5%) | 1.25 steps/sec | ETA: 25.0m
step 3150/5000 (63.0%) | 1.25 steps/sec | ETA: 24.7m
step 3175/5000 (63.5%) | 1.25 steps/sec | ETA: 24.4m
step 3200/5000 (64.0%) | 1.25 steps/sec | ETA: 24.0m
step 3225/5000 (64.5%) | 1.25 steps/sec | ETA: 23.7m
step 3250/5000 (65.0%) | 1.25 steps/sec | ETA: 23.3m
step 3275/5000 (65.5%) | 1.25 steps/sec | ETA: 23.0m
step 3300/5000 (66.0%) | 1.25 steps/sec | ETA: 22.7m
step 3325/5000 (66.5%) | 1.25 steps/sec | ETA: 22.3m
step 3350/5000 (67.0%) | 1.25 steps/sec | ETA: 21.9m
step 3375/5000 (67.5%) | 1.26 steps/sec | ETA: 21.5m
step 3400/5000 (68.0%) | 1.26 steps/sec | ETA: 21.1m
step 3425/5000 (68.5%) | 1.27 steps/sec | ETA: 20.7m
step 3450/5000 (69.0%) | 1.27 steps/sec | ETA: 20.3m
step 3475/5000 (69.5%) | 1.28 steps/sec | ETA: 19.9m
step 3500/5000 (70.0%): train loss 5.9837, val loss 7.3789 | 2732.4s (1.28 steps/sec)
   üíæ Checkpoint saved: checkpoints/checkpoint_gpt2_carolyn_hax_103125_chat_step003500_11112025_140033.pt
step 3525/5000 (70.5%) | 1.29 steps/sec | ETA: 19.1m
step 3550/5000 (71.0%) | 1.29 steps/sec | ETA: 18.7m
step 3575/5000 (71.5%) | 1.30 steps/sec | ETA: 18.3m
step 3600/5000 (72.0%) | 1.30 steps/sec | ETA: 17.9m
step 3625/5000 (72.5%) | 1.31 steps/sec | ETA: 17.6m
step 3650/5000 (73.0%) | 1.31 steps/sec | ETA: 17.2m
step 3675/5000 (73.5%) | 1.32 steps/sec | ETA: 16.8m
step 3700/5000 (74.0%) | 1.32 steps/sec | ETA: 16.4m
step 3725/5000 (74.5%) | 1.32 steps/sec | ETA: 16.0m
step 3750/5000 (75.0%) | 1.33 steps/sec | ETA: 15.7m
step 3775/5000 (75.5%) | 1.33 steps/sec | ETA: 15.3m
step 3800/5000 (76.0%) | 1.34 steps/sec | ETA: 14.9m
step 3825/5000 (76.5%) | 1.34 steps/sec | ETA: 14.6m
step 3850/5000 (77.0%) | 1.35 steps/sec | ETA: 14.2m
step 3875/5000 (77.5%) | 1.35 steps/sec | ETA: 13.9m
step 3900/5000 (78.0%) | 1.36 steps/sec | ETA: 13.5m
step 3925/5000 (78.5%) | 1.36 steps/sec | ETA: 13.2m
step 3950/5000 (79.0%) | 1.37 steps/sec | ETA: 12.8m
step 3975/5000 (79.5%) | 1.37 steps/sec | ETA: 12.5m
step 4000/5000 (80.0%): train loss 5.9417, val loss 7.3308 | 2915.0s (1.37 steps/sec)
   üíæ Checkpoint saved: checkpoints/checkpoint_gpt2_carolyn_hax_103125_chat_step004000_11112025_140335.pt
step 4025/5000 (80.5%) | 1.38 steps/sec | ETA: 11.8m
step 4050/5000 (81.0%) | 1.38 steps/sec | ETA: 11.5m
step 4075/5000 (81.5%) | 1.39 steps/sec | ETA: 11.1m
step 4100/5000 (82.0%) | 1.39 steps/sec | ETA: 10.8m
step 4125/5000 (82.5%) | 1.39 steps/sec | ETA: 10.5m
step 4150/5000 (83.0%) | 1.40 steps/sec | ETA: 10.1m
step 4175/5000 (83.5%) | 1.40 steps/sec | ETA: 9.8m
step 4200/5000 (84.0%) | 1.41 steps/sec | ETA: 9.5m
step 4225/5000 (84.5%) | 1.41 steps/sec | ETA: 9.2m
step 4250/5000 (85.0%) | 1.41 steps/sec | ETA: 8.8m
step 4275/5000 (85.5%) | 1.42 steps/sec | ETA: 8.5m
step 4300/5000 (86.0%) | 1.42 steps/sec | ETA: 8.2m
step 4325/5000 (86.5%) | 1.42 steps/sec | ETA: 7.9m
step 4350/5000 (87.0%) | 1.43 steps/sec | ETA: 7.6m
step 4375/5000 (87.5%) | 1.43 steps/sec | ETA: 7.3m
step 4400/5000 (88.0%) | 1.44 steps/sec | ETA: 7.0m
step 4425/5000 (88.5%) | 1.44 steps/sec | ETA: 6.7m
step 4450/5000 (89.0%) | 1.44 steps/sec | ETA: 6.4m
step 4475/5000 (89.5%) | 1.45 steps/sec | ETA: 6.1m
step 4500/5000 (90.0%): train loss 5.7209, val loss 7.3034 | 3114.5s (1.44 steps/sec)
   üíæ Checkpoint saved: checkpoints/checkpoint_gpt2_carolyn_hax_103125_chat_step004500_11112025_140655.pt
step 4525/5000 (90.5%) | 1.45 steps/sec | ETA: 5.5m
step 4550/5000 (91.0%) | 1.45 steps/sec | ETA: 5.2m
step 4575/5000 (91.5%) | 1.45 steps/sec | ETA: 4.9m
step 4600/5000 (92.0%) | 1.46 steps/sec | ETA: 4.6m
step 4625/5000 (92.5%) | 1.46 steps/sec | ETA: 4.3m
step 4650/5000 (93.0%) | 1.46 steps/sec | ETA: 4.0m
step 4675/5000 (93.5%) | 1.47 steps/sec | ETA: 3.7m
step 4700/5000 (94.0%) | 1.47 steps/sec | ETA: 3.4m
step 4725/5000 (94.5%) | 1.47 steps/sec | ETA: 3.1m
step 4750/5000 (95.0%) | 1.48 steps/sec | ETA: 2.8m
step 4775/5000 (95.5%) | 1.48 steps/sec | ETA: 2.5m
step 4800/5000 (96.0%) | 1.48 steps/sec | ETA: 2.2m
step 4825/5000 (96.5%) | 1.49 steps/sec | ETA: 2.0m
step 4850/5000 (97.0%) | 1.49 steps/sec | ETA: 1.7m
step 4875/5000 (97.5%) | 1.49 steps/sec | ETA: 1.4m
step 4900/5000 (98.0%) | 1.50 steps/sec | ETA: 1.1m
step 4925/5000 (98.5%) | 1.50 steps/sec | ETA: 0.8m
step 4950/5000 (99.0%) | 1.47 steps/sec | ETA: 0.6m
step 4975/5000 (99.5%) | 1.47 steps/sec | ETA: 0.3m
--------------------------------------------------
Training complete! Final loss: 5.9293
Total training time: 1h 12m 59s (1.14 steps/sec)
‚úÖ Final model saved: checkpoints/checkpoint_gpt2_carolyn_hax_103125_chat_step005000_11112025_142759.pt

Generating text...
==================================================
 but but a you I -- I about a. you I butPost.

ice
 to

 to really it -- be is and a for it not to'm we, not and the to all I and don the of the for the to in all it to to people this it the who, like to to you at? out to the. are as canÔøΩ and it as as be the it about the to the and're more the to and it do because are he with the undefined in person otherolyns other, for in, to they for be we the 0 out a this..
GuestGuest? andÔøΩ
 a I to
 of,ÔøΩ. remarks ( your as and doesn thatÔøΩ to're don it the there with therapist I undefined.
 to a.
 isÔøΩ, all for is and of my.
 is
Guest AnyBut it this get with to are I to but
 of you and the into of and
 with in IÔøΩ ( of. my3 they ", and
3. the a other about are the, and there, a because to know will know. (ÔøΩ undefined to
 who

. you but
 the are your and a is it for with is and my and the to. a be the and
 are but the in
. are
ÔøΩ in's as can therapist but about is in
 I This as with the and like and I and
 is this with like, all your we
==================================================
